# grpo-composer

> **A Unified, Component-Driven Library for Critic-Free Reinforcement Learning in Large Language Models**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“– Overview

**grpo-composer** is the first comprehensive, modular library that unifies 22+ GRPO variants into a single framework. By deconstructing the GRPO algorithm into atomic, interchangeable components, researchers can "mix and match" state-of-the-art techniques via simple configuration.

---

## ğŸ“ Repository Structure

```
grpo-composer/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      # This file
â”œâ”€â”€ ğŸ“„ pyproject.toml                 # Package configuration
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md
â”‚
â”œâ”€â”€ ğŸ“‚ configs/                       # YAML configurations
â”‚   â”œâ”€â”€ base_grpo.yaml
â”‚   â”œâ”€â”€ ğŸ“‚ papers/                    # Paper reproduction configs
â”‚   â”‚   â”œâ”€â”€ krpo.yaml
â”‚   â”‚   â”œâ”€â”€ gapo.yaml
â”‚   â”‚   â”œâ”€â”€ dr_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ dapo.yaml
â”‚   â”‚   â”œâ”€â”€ daro.yaml
â”‚   â”‚   â”œâ”€â”€ lambda_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ dra_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ gdpo.yaml
â”‚   â”‚   â”œâ”€â”€ grpo_lead.yaml
â”‚   â”‚   â”œâ”€â”€ ms_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ p_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ pvpo.yaml
â”‚   â”‚   â”œâ”€â”€ rank_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ unlikeliness_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ spo.yaml
â”‚   â”‚   â”œâ”€â”€ stratified_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ tic_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ tr_grpo.yaml
â”‚   â”‚   â”œâ”€â”€ xrpo.yaml
â”‚   â”‚   â”œâ”€â”€ amir_grpo.yaml
â”‚   â”‚   â””â”€â”€ info_grpo.yaml
â”‚   â””â”€â”€ ğŸ“‚ experiments/
â”‚       â”œâ”€â”€ math_reasoning.yaml
â”‚       â”œâ”€â”€ code_generation.yaml
â”‚       â””â”€â”€ agentic_search.yaml
â”‚
â”œâ”€â”€ ğŸ“‚ grpo_composer/                 # ğŸ”¥ Main Package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ version.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ core/                      # Core abstractions
â”‚   â”‚   â”œâ”€â”€ base.py                   # BaseComponent protocols
â”‚   â”‚   â”œâ”€â”€ registry.py               # Component registry
â”‚   â”‚   â”œâ”€â”€ config.py                 # Config management
â”‚   â”‚   â””â”€â”€ trainer.py                # GRPOTrainer
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ rewards/                   # ğŸ¯ Reward Engines (10 modules)
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ binary.py                 # Standard binary
â”‚   â”‚   â”œâ”€â”€ frequency_aware.py        # GAPO
â”‚   â”‚   â”œâ”€â”€ diversity_adjusted.py     # DRA-GRPO
â”‚   â”‚   â”œâ”€â”€ length_dependent.py       # GRPO-LEAD
â”‚   â”‚   â”œâ”€â”€ posterior_composite.py    # P-GRPO
â”‚   â”‚   â”œâ”€â”€ rank_enhanced.py          # RankGRPO
â”‚   â”‚   â”œâ”€â”€ unlikeliness.py           # Unlikeliness-GRPO
â”‚   â”‚   â”œâ”€â”€ rts_based.py              # SPO
â”‚   â”‚   â””â”€â”€ multi_reward.py           # GDPO
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ advantages/                # ğŸ“Š Advantage Estimators (12 modules)
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ standard.py               # (r - Î¼) / Ïƒ
â”‚   â”‚   â”œâ”€â”€ unbiased.py               # Dr.GRPO
â”‚   â”‚   â”œâ”€â”€ kalman.py                 # KRPO
â”‚   â”‚   â”œâ”€â”€ static_value.py           # PVPO
â”‚   â”‚   â”œâ”€â”€ decoupled.py              # GDPO
â”‚   â”‚   â”œâ”€â”€ multi_scale.py            # MS-GRPO
â”‚   â”‚   â”œâ”€â”€ difficulty_aware.py       # GRPO-LEAD
â”‚   â”‚   â”œâ”€â”€ length_corrected.py       # TIC-GRPO
â”‚   â”‚   â”œâ”€â”€ stratified.py             # Stratified-GRPO
â”‚   â”‚   â”œâ”€â”€ advantage_clipping.py     # RankGRPO
â”‚   â”‚   â””â”€â”€ novelty_sharpening.py     # XRPO
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ clipping/                  # âœ‚ï¸ Clipping Mechanisms (5 modules)
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ symmetric.py              # Standard
â”‚   â”‚   â”œâ”€â”€ asymmetric.py             # DAPO
â”‚   â”‚   â”œâ”€â”€ trajectory_level.py       # TIC-GRPO
â”‚   â”‚   â””â”€â”€ weighted_trust.py         # TR-GRPO
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ regularizers/              # ğŸ”— Regularizers (6 modules)
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ kl_divergence.py          # Standard KL
â”‚   â”‚   â”œâ”€â”€ weighted_kl.py            # TR-GRPO
â”‚   â”‚   â”œâ”€â”€ preference.py             # AMIR-GRPO
â”‚   â”‚   â”œâ”€â”€ difficulty_balance.py     # DARO
â”‚   â”‚   â””â”€â”€ info_regularizer.py       # Info-GRPO
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ aggregation/               # âš–ï¸ Token/Group Aggregation (9 modules)
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ token_mean.py             # 1/|o_i|
â”‚   â”‚   â”œâ”€â”€ token_sum.py              # Dr.GRPO
â”‚   â”‚   â”œâ”€â”€ global_token.py           # DAPO
â”‚   â”‚   â”œâ”€â”€ trajectory_level.py       # TIC-GRPO
â”‚   â”‚   â”œâ”€â”€ weighted_token.py         # TR-GRPO
â”‚   â”‚   â”œâ”€â”€ group_uniform.py          # 1/G
â”‚   â”‚   â”œâ”€â”€ group_learnable.py        # Î»-GRPO
â”‚   â”‚   â””â”€â”€ difficulty_weighted.py    # DARO
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ sampling/                  # ğŸ² Sampling Strategies (6 modules)
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ standard.py               # Uniform
â”‚   â”‚   â”œâ”€â”€ dynamic.py                # DAPO
â”‚   â”‚   â”œâ”€â”€ difficulty_grouped.py     # DARO
â”‚   â”‚   â”œâ”€â”€ gt_injection.py           # PVPO
â”‚   â”‚   â””â”€â”€ hierarchical.py           # XRPO
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                    # Model utilities
â”‚   â”‚   â”œâ”€â”€ policy.py
â”‚   â”‚   â”œâ”€â”€ reference.py
â”‚   â”‚   â”œâ”€â”€ reward_model.py
â”‚   â”‚   â””â”€â”€ embeddings.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ losses/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â””â”€â”€ grpo_loss.py              # Unified loss
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/
â”‚       â”œâ”€â”€ logging.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â”œâ”€â”€ checkpointing.py
â”‚       â”œâ”€â”€ distributed.py
â”‚       â””â”€â”€ math_utils.py
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ reproduce_paper.py
â”‚   â””â”€â”€ ablation.py
â”‚
â”œâ”€â”€ ğŸ“‚ examples/
â”‚   â”œâ”€â”€ quickstart.ipynb
â”‚   â”œâ”€â”€ custom_component.py
â”‚   â””â”€â”€ mix_and_match.py
â”‚
â”œâ”€â”€ ğŸ“‚ tests/
â”‚   â”œâ”€â”€ test_rewards/
â”‚   â”œâ”€â”€ test_advantages/
â”‚   â”œâ”€â”€ test_clipping/
â”‚   â”œâ”€â”€ test_regularizers/
â”‚   â”œâ”€â”€ test_aggregation/
â”‚   â”œâ”€â”€ test_sampling/
â”‚   â”œâ”€â”€ test_integration/
â”‚   â””â”€â”€ test_paper_recovery/          # Exact paper reproduction tests
â”‚
â”œâ”€â”€ ğŸ“‚ benchmarks/
â”‚   â”œâ”€â”€ math_reasoning/
â”‚   â”œâ”€â”€ code_generation/
â”‚   â””â”€â”€ memory_profiling/
â”‚
â””â”€â”€ ğŸ“‚ docs/
    â”œâ”€â”€ index.md
    â”œâ”€â”€ getting_started.md
    â”œâ”€â”€ ğŸ“‚ concepts/
    â”œâ”€â”€ ğŸ“‚ papers/
    â”œâ”€â”€ ğŸ“‚ api/
    â””â”€â”€ ğŸ“‚ tutorials/
```

---

## ğŸ—ºï¸ Component Mapping (22 Papers)

| Paper | Reward | Advantage | Clipping | Regularizer | Aggregation | Sampling |
|:------|:------:|:---------:|:--------:|:-----------:|:-----------:|:--------:|
| **GRPO** | binary | standard | symmetric | kl | token_mean | standard |
| **KRPO** | - | kalman | - | - | - | - |
| **GAPO** | frequency | - | - | - | - | - |
| **Dr.GRPO** | - | unbiased | - | none | token_sum | - |
| **DRA-GRPO** | diversity | - | - | - | - | - |
| **DAPO** | - | - | asymmetric | none | global_token | dynamic |
| **DARO** | - | - | - | difficulty | difficulty_wt | difficulty |
| **Î»-GRPO** | - | - | - | - | learnable | - |
| **GDPO** | multi | decoupled | - | - | - | - |
| **GRPO-LEAD** | length | difficulty | - | none | - | - |
| **MS-GRPO** | - | multi_scale | - | - | - | - |
| **P-GRPO** | composite | - | asymmetric | none | - | - |
| **PVPO** | - | static_v | - | - | - | gt_inject |
| **RankGRPO** | rank | adv_clip | - | - | - | - |
| **Unlikeliness** | unlikely | - | - | - | - | - |
| **SPO** | rts | - | - | - | - | - |
| **Stratified** | - | stratified | - | - | - | - |
| **TIC-GRPO** | - | length_corr | trajectory | - | trajectory | - |
| **TR-GRPO** | - | - | weighted | weighted_kl | weighted | - |
| **XRPO** | - | novelty | - | - | - | hierarchical |
| **AMIR-GRPO** | - | - | - | preference | - | - |
| **Info-GRPO** | - | - | - | info | - | - |

---

## ğŸš€ Quick Start

### Installation
```bash
pip install grpo-composer
```

### Basic Usage
```python
from grpo_composer import GRPOTrainer
from grpo_composer.rewards import FrequencyAwareReward
from grpo_composer.advantages import KalmanAdvantage

# Mix GAPO's reward with KRPO's advantage
trainer = GRPOTrainer(
    model=model,
    reward_engine=FrequencyAwareReward(),       # GAPO
    advantage_estimator=KalmanAdvantage(),      # KRPO
)
trainer.train(dataset)
```

### Via Config
```yaml
reward_engine: frequency_aware      # GAPO
advantage_estimator: kalman         # KRPO
clipping: asymmetric                # DAPO
regularizer: preference             # AMIR-GRPO
```

```bash
python scripts/train.py --config configs/custom.yaml
```

### Reproduce a Paper
```bash
python scripts/reproduce_paper.py --paper krpo
```

---

## ğŸ“š Supported Papers (22)

1. **GRPO** - DeepSeekMath (2024)
2. **KRPO** - Kalman Filter Posterior
3. **GAPO** - Group-Aware Frequency Rewards
4. **Dr.GRPO** - Bias-Free Gradients
5. **DRA-GRPO** - Diversity via SMI
6. **DAPO** - Asymmetric Clipping
7. **DARO** - Difficulty-Aware Weighting
8. **Î»-GRPO** - Learnable Length Weights
9. **GDPO** - Multi-Reward Decoupling
10. **GRPO-LEAD** - Length + Difficulty
11. **MS-GRPO** - Multi-Scale Advantages
12. **P-GRPO** - Posterior Thinking Reward
13. **PVPO** - Static Value Baseline
14. **RankGRPO** - Ranking as Reward
15. **Unlikeliness-GRPO** - Rare Solution Boost
16. **SPO** - Reasoning Trajectory Score
17. **Stratified-GRPO** - Per-Stratum Normalization
18. **TIC-GRPO** - Trajectory-Level Importance
19. **TR-GRPO** - Token-Regulated Sharpness
20. **XRPO** - Exploration-Exploitation Planning
21. **AMIR-GRPO** - DPO-Style Preference
22. **Info-GRPO** - Mutual Information Regularizer

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.
